{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 on Machine Learning\n",
    "\n",
    "## Classification and Regression, from linear and logistic regression to neural networks\n",
    "\n",
    " Jessica Alatorre Flores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "In this project we will study regression and classification problems, starting with the same algorithm we studied in project 1, then we will inlcude logistic regression for classification problem, and finally we will implement a multilayer perceptron model for both problems, regression and classification.\n",
    "\n",
    "We will be working with the data from the Ising model and we will focus on supervised training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part a) Producing the data for the one-dimensional Ising model\n",
    "\n",
    "\"The Ising model, is a mathematical model of ferromagnetism in statistical mechanics. The model consists of discrete variables that represent magnetic dipole moments of atomic spins that can be in one of two states (+1 or −1). The spins are arranged in a graph, usually a lattice, allowing each spin to interact with its neighbors. The model allows the identification of phase transitions, as a simplified model of reality\" (https://en.wikipedia.org/wiki/Ising_model)\n",
    "\n",
    "For the discussion here, we will use the one-dimensional Ising model that consists of a simple binary value system where the variables of the model (spins) can take two values only, for example (+1, -1) or (0, 1).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "E = -J\\sum_{\\langle kj\\rangle}S_{i}S_j\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the one-dimensional Ising model with nearest neighbor iteraction, this model has no phase transition at finite temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Python code generates the training data, and defines the Ising model parameters. \n",
    "This codes is based in the one that was provided to us in the poject specifitations. (https://compphysics.github.io/MachineLearning/doc/Projects/2018/Project2/pdf/Project2.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(12)\n",
    "\n",
    "def ising_energies(states,L):\n",
    "    \"\"\"\n",
    "    This function calculates the energies of the states in the nn Ising Hamiltonian\n",
    "    \"\"\"\n",
    "    J=np.zeros((L,L),)\n",
    "    for i in range(L):\n",
    "        J[i,(i+1)%L]-=1.0\n",
    "    # compute energies\n",
    "    E = np.einsum('...i,ij,...j->...',states,J,states)\n",
    "    return E\n",
    "\n",
    "\n",
    "\"\"\" Define Ising model \"\"\"\n",
    "#system size\n",
    "L = 40\n",
    "#create 1000 random Ising states\n",
    "states = np.random.choice([-1, 1], size=(1000, L))\n",
    "\n",
    "# calculate Ising energies\n",
    "energies=ising_energies(states,L)  # Y or dependent var\n",
    "\n",
    "# reshape Ising states into RL samples: S_iS_j --> X_p\n",
    "states=np.einsum('...i,...j->...ij', states, states)\n",
    "shape=states.shape\n",
    "states=states.reshape((shape[0],shape[1]*shape[2])) # X or independent var\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply Linear regression, we have to recast the moddel in the form:\n",
    "\\begin{equation*}\n",
    "H_\\mathrm{model}^i \\equiv \\mathbf{X}^i \\cdot \\mathbf{J},\n",
    "\\end{equation*}\n",
    "\n",
    "where the vectors $X_i$\n",
    "represent all two-body interactions $\\{S_{j}^iS_{k}^i \\}_{j,k=1}^L$ , and the index $i$ runs over the samples in the data set. To make the analogy complete, we\n",
    "can also represent the dot product by a single index $p = \\{j,k\\}$, i.e.$\\mathbf{X}^i \\cdot \\mathbf{J}=X^i_pJ_p$\n",
    "Note that the regression model does not include the minus sign, so we expect tolearn negative $J’s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part b) Estimating the coupling constant of the one-dimensional Ising model using linear regression\n",
    "\n",
    "In this part I used the same codes from project 1, but this time with a clearer structure and separating into functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Least square regression\n",
    "def ols_regression(data_train, data_test, depen):\n",
    "    \"\"\" Function that performs the OLS regression with the inverse\"\"\"\n",
    "    beta_ols = np.linalg.inv(data_train.T @ data_train) @ data_train.T @ depen\n",
    "    pred= data_test@beta_ols\n",
    "    return beta_ols, pred\n",
    "\n",
    "import scipy.linalg as scl\n",
    "def ols_SVD(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Function that performs the OLS regression with the Singular Value Descomposition\"\"\"\n",
    "    u, s, v = scl.svd(x)\n",
    "    return v.T @ scl.pinv(scl.diagsvd(s, u.shape[0], v.shape[0])) @ u.T @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(data_train, data_test, depen, alpha):\n",
    "    \"\"\"Funtions that performs the Ridge Regression\"\"\"\n",
    "    beta_olsRidge = np.linalg.inv(data_train.T @ data_train + alpha*np.identity(1600)) @ data_train.T @ depen\n",
    "    pred= data_test@beta_olsRidge\n",
    "    return beta_olsRidge, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The following functions calculate different measurements to assess the model\"\"\"\n",
    "def mse(y_pred, y_test):\n",
    "    return np.mean( np.mean((y_test - y_pred)**2) )\n",
    "def r2(y_pred, y_test):\n",
    "    return np.sqrt(np.mean( np.mean((y_test - y_pred)**2) ))\n",
    "def bias(y_pred, y_test):\n",
    "    return np.mean( (y_test - np.mean(y_pred))**2 )\n",
    "def variance(y_pred, y_test):\n",
    "    return np.mean( np.var(y_pred)) \n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "def all_values(y_pred, y_test):\n",
    "    measuremnts = [['MSE', mse(y_pred,y_test)],\n",
    "         ['R2', r2(y_pred,y_test)],\n",
    "         ['Bias', bias(y_pred,y_test)],\n",
    "         ['Variance', variance(y_pred,y_test)]]\n",
    "    return measuremnts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After define the function we need, the data was splitted in train and test using sklear.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(states, energies, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing the linear regression with single value descomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--OLS with SVD--\n",
      "--------  ------------\n",
      "MSE        0.000592563\n",
      "R2         0.0243426\n",
      "Bias      36.2716\n",
      "Variance  36.2368\n",
      "--------  ------------\n"
     ]
    }
   ],
   "source": [
    "coefs_OLS = ols_SVD(x_train, y_train)\n",
    "y_pred= x_test@coefs_OLS\n",
    "\n",
    "print('\\n--OLS with SVD--')\n",
    "print(tabulate(all_values(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Ridge Regression with different values for lamda and choosing the one with the best result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--Ridge regression--\n",
      "The best value for alpha =  0.0001\n",
      "--------  ------------\n",
      "MSE        7.68038e-09\n",
      "R2         8.76378e-05\n",
      "Bias      36.2716\n",
      "Variance  36.2715\n",
      "--------  ------------\n"
     ]
    }
   ],
   "source": [
    "alphas= np.logspace(-4, 5, 10)\n",
    "MSE_R = 1\n",
    "for val in alphas:\n",
    "    beta, prediRidge = ridge_regression(x_train, x_test, y_train, val)\n",
    "    mseRidge = mse(prediRidge, y_test)\n",
    "    if (mseRidge <= MSE_R):\n",
    "        alphaR= val\n",
    "        Beta_R= beta\n",
    "        pred_Ridge = prediRidge\n",
    "        MSE_R = mseRidge\n",
    "        \n",
    "#Print results       \n",
    "print('\\n--Ridge regression--')\n",
    "print('The best value for alpha = ', alphaR)  \n",
    "print(tabulate(all_values(pred_Ridge,y_test)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Lasso Regression using sklear because this is the way we did in project 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--Lasso regression--\n",
      "The best value for alpha =  0.0001\n",
      "--------  ------------\n",
      "MSE        1.59884e-06\n",
      "R2         0.00126445\n",
      "Bias      36.2716\n",
      "Variance  36.2656\n",
      "--------  ------------\n"
     ]
    }
   ],
   "source": [
    "#--------------Lasso Regression-------------\n",
    "from sklearn.linear_model import Lasso #Using Sklearn as I did in 1st project\n",
    "alphasL= np.logspace(-4, 5, 10)\n",
    "regr = Lasso()\n",
    "scores = [regr.set_params(alpha = alpha).fit(x_train, y_train).score(x_test, y_test) for alpha in alphasL]\n",
    "best_alpha = alphasL[scores.index(max(scores))]\n",
    "regr.alpha = best_alpha\n",
    "regr.fit(x_train, y_train)\n",
    "pred_Lasso = regr.predict(x_test)\n",
    "\n",
    "#Print results       \n",
    "print('\\n--Lasso regression--')\n",
    "print('The best value for alpha = ', best_alpha)  \n",
    "print(tabulate(all_values(pred_Lasso,y_test))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part c) Determine the phase of the two-dimensional Ising model\n",
    "\n",
    "Now, we will use the two-dimensional Ising Model, and we will use the data sets generated by d by [Mehta et al](https://physics.bu.edu/~pankajm/ML-Review-Datasets/isingMC/). \n",
    "\n",
    "We will use a fixed lattice of L × L = 40 × 40 spins in two dimensions.\n",
    "\n",
    "The aim of this section is to use logistic regression to train our model and predict the phase of a sample given the spin configuration, wheter it represents a order or a disorder state. Is it an order state when it is below the critical temperature, and a disordered state when it is above this temperature. The theoretical critical\n",
    "temperature for a phase transition is $TC ≈ 2.269$ in units of energy. \n",
    "\n",
    "The algorithm to resolve this part was:\n",
    "\n",
    "  1.- Read the data (based in the code from [Metha et all](https://physics.bu.edu/~pankajm/ML-Notebooks/HTML/NB_CVII-logreg_ising.html).\n",
    "  \n",
    "  2.- Write the code to perform Logistic Regression:\n",
    "  \n",
    "      i) Given a set of inputs, assign them to a category\n",
    "   \n",
    "      ii) Genetare the probabilities with a function that gives outputs between 0 and 1. (Sigmoid function)\n",
    "   \n",
    "      iii) Define a function that give us the parameters/weights. -> Cost function \n",
    "   \n",
    "      iv) In order to minimize the cost function we increse/decrese the weigths with the derivate of the loss function with respect to each weight. (Gradient Descent)\n",
    "  \n",
    "       vi) Update the weights and repeat until reach the optimal.\n",
    "   \n",
    "       vii) Predict the output using the sigmoid function\n",
    "   \n",
    "   3.- Evaluate the model using the accuracy score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$Accuracy = \\frac{\\sum_{k=1}^n I(t_i = y_i)}{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code and the theory discussed above is based on  https://medium.com/@martinpella/logistic-regression-from-scratch-in-python-124c5636b8ac and also on the lectures notes on [logistic regression](https://compphysics.github.io/MachineLearning/doc/pub/LogReg/html/LogReg-bs.html)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.model_selection as skms\n",
    "import pickle,os, glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, verbose=False):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        # weights initialization\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        #Standard Gradient Descent\n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "              \n",
    "    def predict_prob(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.predict_prob(X).round()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ising model\n",
    "L=40 # linear system size\n",
    "J=-1.0 # Ising interaction\n",
    "T=np.linspace(0.25,4.0,16) # set of temperatures\n",
    "#Read the files for the 2D data\n",
    "filenames = glob.glob(os.path.join(\"..\", \"dat\", \"*\"))\n",
    "label_filename = \"Ising2DFM_reSample_L40_T=All_labels.pkl\"\n",
    "dat_filename = \"Ising2DFM_reSample_L40_T=All.pkl\"\n",
    "file_name = \"Ising2DFM_reSample_L40_T=All.pkl\"\n",
    "# Read in the labels\n",
    "with open(label_filename, \"rb\") as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "# Read in the corresponding configurations\n",
    "with open(dat_filename, \"rb\") as f:\n",
    "    data = np.unpackbits(pickle.load(f)).reshape(-1, 1600).astype(\"int\")\n",
    "\n",
    "# Set spin-down to -1\n",
    "data[data == 0] = -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the train and test sets with the corresponding slices of the data set for the ordered and disordered phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up slices of the dataset\n",
    "ordered = slice(0, 70000)\n",
    "critical = slice(70000, 100000)\n",
    "disordered = slice(100000, 160000)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = skms.train_test_split(\n",
    "    np.concatenate((data[ordered], data[disordered])),\n",
    "    np.concatenate((labels[ordered], labels[disordered])),\n",
    "    test_size=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the object of the class LogisticRegression with a learning rate = 0.1 and 100000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47640485829959517\n",
      "[ 8.25215081  0.6803206  -1.94657074 ... -0.90526201  0.78357407\n",
      " -2.06865464]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.1, num_iter=100000)\n",
    "model.fit(X_train,Y_train)\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "accuracy = (preds == Y_test).mean()\n",
    "print(accuracy)\n",
    "print(model.theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About the previous results:\n",
    "I got a very low accuracy (lower than 50%) which means that my model it is not very good, but it was interesting see that I got different results here from the iPhyton console, where I got and accuracy = 0.7011, which is closer and also higher that the accuracy I got using sklearn. So maybe it could be somthing with jupyter notebook and not with the model itself.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.694834008097166\n",
      "[1.3309357] [[ 0.10347112 -0.36762477  0.3178758  ... -0.21941697  0.1148904\n",
      "  -0.43908601]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "modelS = LogisticRegression(C=1e20)\n",
    "modelS.fit(X_train, Y_train)\n",
    "preds = modelS.predict(X_test)\n",
    "accuracyS = (preds == Y_test).mean()\n",
    "print(accuracyS)\n",
    "print(modelS.intercept_, modelS.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part d) Regression analysis of the one-dimensional Ising model using neural networks\n",
    "\n",
    "The goal now is to write a code that perform a multilayer perceptron model, implementing backpropagation algorithm.\n",
    "\n",
    "\n",
    "\"A multilayer perceptron (MLP) is a class of feedforward artificial neural network. An MLP consists of, at least, three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable\" [1](https://en.wikipedia.org/wiki/Multilayer_perceptron)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm to perform Backpropagation to train the model is:\n",
    "\n",
    "1.- Inizialize the network collecting and pre-processing the data\n",
    "\n",
    "2.- Define the model and architecture\n",
    "\n",
    "3.- Propagate the network using feed forward\n",
    "\n",
    "4.- Choose a cost function and an optimizer\n",
    "\n",
    "5.- Train the Network\n",
    "\n",
    "6.- Compute the back-propagate errors\n",
    "\n",
    "7.- Predict the values for the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    def __init__(self,  X_dat, Y_dat,epochs=10,\n",
    "        batch_size=100,\n",
    "        eta=0.1,\n",
    "        lmbd=0.0):\n",
    "        #parameters\n",
    "        self.X_data_full = X_dat\n",
    "        self.Y_data_full = Y_dat\n",
    "        \n",
    "        self.inputSize, self.n_features = X_dat.shape\n",
    "        self.outputSize = 10\n",
    "        self.hiddenSize = 50\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.iterations = self.inputSize // self.batch_size\n",
    "        self.eta = eta\n",
    "        self.lmbd = lmbd\n",
    "    \n",
    "        #weights\n",
    "        self.W1 = np.random.randn(self.n_features, self.hiddenSize) \n",
    "        self.W2 = np.random.randn(self.hiddenSize, self.outputSize) \n",
    "        #bias\n",
    "        self.B1 = np.zeros(self.hiddenSize) + .01 \n",
    "        self.B2 = np.zeros(self.outputSize) + .01 \n",
    "    \n",
    "        self.weights = [self.W1, self.W2]\n",
    "        self.biases = [self.B1, self.B2]\n",
    "\n",
    "    def sigmoid(self, s):\n",
    "        return 1/(1+np.exp(-s))\n",
    "\n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def feed_forward(self):\n",
    "        # feed-forward for training\n",
    "        self.z_h = np.matmul(self.X_dat, self.W1) + self.B1\n",
    "        self.a_h = self.sigmoid(self.z_h)\n",
    "        self.z_o = np.matmul(self.a_h, self.W2) + self.B2\n",
    "        self.probabilities = self.sigmoid(self.z_o)\n",
    "        exp_term = np.exp(self.z_o)\n",
    "        np.seterr(divide='ignore', invalid='ignore')\n",
    "        self.probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "\n",
    "    def feed_forward_out(self, X):\n",
    "      # feed-forward for output\n",
    "      z_h = np.matmul(X, self.W1) + self.B1\n",
    "      a_h = self.sigmoid(z_h)\n",
    "      \n",
    "      z_o = np.matmul(a_h, self.W2) + self.B2\n",
    "      exp_term = np.exp(z_o)\n",
    "      \n",
    "      probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "      return probabilities\n",
    "\n",
    "    def backpropagation(self):\n",
    "        # error in the output layer\n",
    "        error_output = self.probabilities - self.Y_dat[0]\n",
    "        # error in the hidden layer\n",
    "        error_hidden = np.matmul(error_output, self.W2.T) * self.a_h * (1 -self.a_h)\n",
    "        \n",
    "        self.output_weights_gradient = np.matmul(self.a_h.T, error_output)\n",
    "        self.output_bias_gradient = np.sum(error_output, axis=0)\n",
    "        \n",
    "        self.hidden_weights_gradient = np.matmul(self.X_dat.T, error_hidden)\n",
    "        self.hidden_bias_gradient = np.sum(error_hidden, axis=0)\n",
    "        \n",
    "        if self.lmbd > 0.0:\n",
    "                self.output_weights_gradient += self.lmbd * self.W2\n",
    "                self.hidden_weights_gradient += self.lmbd * self.W1\n",
    "    \n",
    "        self.W2 -= self.eta * self.output_weights_gradient\n",
    "        self.B2 -= self.eta * self.output_bias_gradient\n",
    "        self.W1 -= self.eta * self.hidden_weights_gradient\n",
    "        self.B1 -= self.eta * self.hidden_bias_gradient\n",
    "  \n",
    "    def predict(self, X):\n",
    "        probabilities = self.feed_forward_out(X)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "    def train(self):\n",
    "        data_indices = np.arange(self.inputSize)\n",
    "        for i in range(self.epochs):\n",
    "            for j in range(self.iterations):\n",
    "                # pick datapoints with replacement\n",
    "                chosen_datapoints = np.random.choice(\n",
    "                    data_indices, size=self.batch_size, replace=False)\n",
    "                # minibatch training data\n",
    "                self.X_dat = self.X_data_full[chosen_datapoints]\n",
    "                self.Y_dat = self.Y_data_full[chosen_datapoints]\n",
    "\n",
    "                self.feed_forward()\n",
    "                self.backpropagation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 100\n",
    "eta = 0.01 #learning rate\n",
    "lmbd = 0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the Neural_Network class and train the model to predict the values for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = Neural_Network(x_train, y_train, epochs=epochs, batch_size=batch_size, eta=eta,  lmbd=lmbd)\n",
    "    \n",
    "dnn.train()\n",
    "test_predict = dnn.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that compute the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_test, Y_pred):\n",
    "    return np.sum(y_test == Y_pred) / len(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on test set:  0.235\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score on test set: \", accuracy_score(y_test, test_predict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About the previous result\n",
    "I got a very small Accuracy score and I think is due the calculation of the probabilies when doing the forward propagation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Again I wasn´t able to finish all the project because I got a lot of trouble in the understanding of some calculations. And since this is my first course programing this way, I get very delayed because many simple programming stuff. I have to be checking a lot the documentation and tutorial to program. \n",
    "\n",
    "I understood well how a multilayer perceptron model works but to be honest it was to set all the things I undertood in a program.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
